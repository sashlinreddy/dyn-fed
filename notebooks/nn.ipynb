{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from fault_tolerant_ml.data.mnist import MNist\n",
    "from fault_tolerant_ml.models import nn\n",
    "import fault_tolerant_ml.activations.activation as F\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "filepaths = {\n",
    "    \"train\": {\n",
    "        \"images\": os.path.join(data_dir, \"train-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"train-labels-idx1-ubyte.gz\")\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"images\": os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"t10k-labels-idx1-ubyte.gz\")\n",
    "    }\n",
    "}\n",
    "mnist = MNist(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MNist X_train=(60000, 784), y_train=(60000, 10), X_test=(10000, 784), y_test=(10000, 10)>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features=784, n_classes=10\n"
     ]
    }
   ],
   "source": [
    "n_features, n_classes = mnist.X_train.shape[1], mnist.y_train.shape[1]\n",
    "print(f\"n_features={n_features}, n_classes={n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.randn(n_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # MLP - 1 input layer, 1 hidden layer, 1 output layer\n",
    "        # self.fc1 = nn.Layer(n_inputs=784, n_outputs=128)\n",
    "        # self.fc2 = nn.Layer(n_inputs=128, n_outputs=10)\n",
    "        self.layers = []\n",
    "        # self.act_fn = F.Sigmoid()\n",
    "        \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add layer to model\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "        return self\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # z1 = self.fc1(x)\n",
    "        # a1 = self.act_fn(z1)\n",
    "        # a1, z1 = self.fc1(x)\n",
    "        # z2 = self.fc2(a1)\n",
    "        # y_pred = self.act_fn(z2)\n",
    "        # y_pred, z2 = self.fc2(a1)\n",
    "        \n",
    "        a = x\n",
    "        for layer in self.layers:\n",
    "            print(layer)\n",
    "            layer(a)\n",
    "            a = layer.y\n",
    "        \n",
    "        y_pred = a\n",
    "        return y_pred\n",
    "    \n",
    "    def backward():\n",
    "        pass\n",
    "    \n",
    "#     def backward(self, x, y, a_n):\n",
    "        \n",
    "#         y_pred, z2, a1, z1 = a_n\n",
    "#         # Output layer error\n",
    "#         delta2 = (y_pred - y)# * self.act_fn.grad(z2)\n",
    "#         # Gradient of cost function\n",
    "#         dw2 = np.dot(a1.T, delta2)\n",
    "#         # Backpropagate the error through the network\n",
    "#         delta1 = np.dot(delta2, self.fc2.W.T) * self.act_fn.grad(z1)\n",
    "#         # Calculate gradient\n",
    "#         dw1 = np.dot(x.T, delta1)\n",
    "#         # Gradient of biases equal to the error\n",
    "#         db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "#         db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "#         return dw2, db2, dw1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y):\n",
    "    return np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y, y_pred):\n",
    "    y_pred_ = y_pred.argmax(axis=1)\n",
    "    y_ = y.argmax(axis=1)\n",
    "    return np.sum(y_pred_==y_) / y_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.ml.ops.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tensor(mnist.X_train)\n",
    "W = Tensor(np.random.randn(784, 128))\n",
    "b = Tensor(np.random.randn(1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Tensor(np.array(3), requires_grad=True)\n",
    "x2 = Tensor(np.array(7), requires_grad=False)\n",
    "\n",
    "interim = ((x1 + x1) + x2)\n",
    "f = interim * interim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(2x_1 + x_2)^2 = 4x_1^2 + 4x_1x_2 +x_2^2$ \n",
    "\n",
    "$\\frac{\\partial{df}}{\\partial{x_1}} =8x_1 + 4x_2 = 24 + 28 = 52$\n",
    "\n",
    "$\\frac{\\partial{df}}{\\partial{x_2}} =4x_1 + 2x_2 = 12 + 14 = 26$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(52.0, requires_grad=False)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer(n_inputs=784, n_outputs=128)\n",
      "Layer(n_inputs=128, n_outputs=10)\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "model.add(nn.Layer(n_inputs=784, n_outputs=128))\n",
    "model.add(nn.Layer(n_inputs=128, n_outputs=10))\n",
    "y_pred = model.forward(mnist.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer(n_inputs=128, n_outputs=10)\n",
      "Layer(n_inputs=784, n_outputs=128)\n"
     ]
    }
   ],
   "source": [
    "l = \n",
    "for layer in model.layers[::-1]:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNet()\n",
    "# print(model.fc1.shape)\n",
    "# print(model.fc2.shape)\n",
    "# epochs = 400\n",
    "# learning_rate = 0.99\n",
    "# m = mnist.X_train.shape[0]\n",
    "# for epoch in np.arange(epochs):\n",
    "    \n",
    "#     # Feedforward\n",
    "#     y_pred, z2, a1, z1 = model.forward(mnist.X_train)\n",
    "    \n",
    "#     # Calculate cost\n",
    "#     loss = cross_entropy_loss(y_pred, mnist.y_train)\n",
    "    \n",
    "#     # Backprop\n",
    "#     dw2, db2, dw1, db1 = model.backward(mnist.X_train, mnist.y_train, [y_pred, z2, a1, z1])\n",
    "    \n",
    "#     # Update weights\n",
    "#     model.fc2.W = model.fc2.W - learning_rate * 1 / m * dw2\n",
    "#     model.fc1.W = model.fc1.W - learning_rate * 1 / m * dw1\n",
    "#     model.fc2.b = model.fc2.b - learning_rate * 1 / m * db2\n",
    "#     model.fc1.b = model.fc1.b - learning_rate * 1 / m * db1\n",
    "    \n",
    "#     acc = accuracy_score(mnist.y_train, y_pred)\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'epoch = {epoch}, loss = {loss:.3f}, TRAIN ACC = {acc:.3f}')\n",
    "#     epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = Graph()\n",
    "\n",
    "# g.set_as_default()\n",
    "\n",
    "# X = Tensor(mnist.X_train)\n",
    "\n",
    "# X + W\n",
    "\n",
    "# W = Variable(np.random.rand(784, 128))\n",
    "\n",
    "# b = Variable(np.zeros(shape=(1, 128)))\n",
    "\n",
    "# z = add(matmul(X, W), b)\n",
    "\n",
    "# z.input_nodes[0].input_nodes[0]\n",
    "\n",
    "# z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.ml.ops import tensor as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ft.Graph()\n",
    "g.set_as_default()\n",
    "X = ft.Tensor(mnist.X_train)\n",
    "y = ft.Tensor(mnist.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = ft.Tensor(np.random.randn(784, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ft.Tensor(np.zeros((1, 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ft.matmul(X, W)\n",
    "# z = ft.add(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<fault_tolerant_ml.ml.ops.tensor.matmul at 0x12f6ce630>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalulate(f):\n",
    "    val = []\n",
    "    for i, op in enumerate(f.operations):\n",
    "        print(*op.input_nodes)\n",
    "        val.append(op.compute(*op.input_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(f):\n",
    "    \n",
    "    operations = []\n",
    "    def recurse(node):\n",
    "        if isinstance(node, ft.Operation):\n",
    "            for input_node in node.input_nodes:\n",
    "                recurse(input_node)\n",
    "        operations.append(node)\n",
    "            \n",
    "    recurse(f)\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ft.Graph()\n",
    "g.set_as_default()\n",
    "x1 = ft.Tensor(np.array(3))\n",
    "x2 = ft.Tensor(np.array(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ft.square(ft.add(ft.add(x1, x1), x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(3, dtype=int64),\n",
       " Tensor(3, dtype=int64),\n",
       " add(),\n",
       " Tensor(7, dtype=int64),\n",
       " add(),\n",
       " square()]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traverse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(3, dtype=int64), Tensor(3, dtype=int64)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.operations[0].input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalulate(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [ (\"z1\", \"add\", (\"x1\",\"x1\")),\n",
    "# (\"z2\", \"add\", (\"z1\",\"x2\")),\n",
    "# (\"f\", \"square\", (\"z2\",)) ]\n",
    "\n",
    "# G = { \"add\" : lambda a,b: a+b,\n",
    "# \"square\": lambda a:a*a }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = { \"x1\" : 3, \"x2\" : 7 }\n",
    "\n",
    "# for step in l:\n",
    "#     print(val)\n",
    "#     var, op_name, func = step\n",
    "#     lookup = list(map(val.get, func))\n",
    "#     val[var] = G[op_name](*lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DG = { \"add\" : [ (lambda a,b: 1), (lambda a,b: 1) ],\n",
    "# \"square\": [ lambda a:2*a ] }\n",
    "\n",
    "# delta={}\n",
    "# delta[\"f\"] = 1\n",
    "# for step in l[::-1]:\n",
    "#     var, op_name, func = step\n",
    "#     for op in DG[op_name]:\n",
    "#         if var not in delta:\n",
    "#             delta[var] = 0\n",
    "#         lookup = list(map(val.get, func))\n",
    "#         print(lookup)\n",
    "#         delta[var] = delta[var] + DG[op_name](*lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, depends_on=None):\n",
    "        \n",
    "        self.depends_on = depends_on or []\n",
    "        self.data = data\n",
    "            \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data, depends_on=[self, other])\n",
    "    \n",
    "    def __pow__(self, p):\n",
    "        data = self.data ** p \n",
    "        return Tensor(data, depends_on=[self])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.data}, dtype={self.data.dtype})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = Tensor(np.array(3))\n",
    "y2 = Tensor(np.array(7))\n",
    "z1 = y1 + y1\n",
    "z2 = z1 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(6, dtype=int64)]"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2.depends_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.activations.activation import Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_array(arrayable):\n",
    "    if isinstance(arrayable, np.ndarray):\n",
    "        return arrayable\n",
    "    else:\n",
    "        return np.array(arrayable)\n",
    "\n",
    "Tensorable = Union['Tensor', 'float', np.ndarray]\n",
    "\n",
    "def ensure_tensor(tensorable: Tensorable):\n",
    "    if isinstance(tensorable, Tensor):\n",
    "        return tensorable\n",
    "    else:\n",
    "        return Tensor(tensorable)\n",
    "    \n",
    "class Tensor():\n",
    "    \n",
    "    def __init__(self, data, is_param=False):\n",
    "        self.data = ensure_array(data)\n",
    "        self._is_param = is_param\n",
    "        self._grad = None\n",
    "        self.shape = self.data.shape\n",
    "        self.ndim = self.data.ndim\n",
    "        \n",
    "        if is_param:\n",
    "            self.zero_grad()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.data}, parameter={self.is_param})\"\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        return Tensor(self.data @ ensure_tensor(other).data)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + ensure_tensor(other).data)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return Tensor(self.data + ensure_tensor(other).data)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return Tensor(self.data - ensure_tensor(other).data)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return Tensor(ensure_tensor(other).data - self.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return Tensor(self.data * ensure_tensor(other).data)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return Tensor(ensure_tensor(other).data * self.data)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return Tensor(self.data / ensure_tensor(other).data)\n",
    "        \n",
    "    def __rtruediv__(self, other):\n",
    "        return Tensor(ensure_tensor(other).data/ self.data)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return Tensor(-self.data)\n",
    "    \n",
    "    def __getitem__(self, idxs):\n",
    "        return Tensor(self.data[idxs])\n",
    "    \n",
    "    def exp(self):\n",
    "        return Tensor(np.exp(self.data))\n",
    "    \n",
    "    def log(self):\n",
    "        return Tensor(np.log(self.data))\n",
    "    \n",
    "    def mean(self, axis=None, dtype=None, out=None):\n",
    "        if dtype is None:\n",
    "            dtype = self.data.dtype\n",
    "        return Tensor(np.mean(self.data, axis=axis, dtype=dtype, out=out))\n",
    "    \n",
    "    def sum(self, axis=None, out=None, **passkwargs):\n",
    "        return Tensor(np.sum(self.data, axis=axis, out=out, **passkwargs))\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return Tensor(self.data.T)\n",
    "    \n",
    "    @property\n",
    "    def is_param(self):\n",
    "        return self._is_param\n",
    "    \n",
    "    @property\n",
    "    def grad(self):\n",
    "        return self._grad\n",
    "    \n",
    "    @grad.setter\n",
    "    def grad(self, grad):\n",
    "        self._grad = ensure_tensor(grad)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = Tensor(np.zeros_like(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lay():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.W = Tensor((np.random.randn(self.n_inputs, self.n_outputs) * 0.01).astype(np.float32), is_param=True)\n",
    "        self.b = Tensor(np.zeros((1, self.n_outputs)).astype(np.float32), is_param=True)\n",
    "        \n",
    "        self.activation_fn = Sigmoid()\n",
    "        self.derived_vars = []\n",
    "        \n",
    "    def __call__(self, value):\n",
    "        # Store input tensor for feedforward\n",
    "        self.x = value\n",
    "        # Store edge\n",
    "        self.z = (value @ self.W) + self.b\n",
    "        # Store output tensor for feedforward\n",
    "        self.y = Tensor(self.activation_fn(self.z.data))\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Layer({self.n_inputs}, {self.n_outputs})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Model([{self.layers}])\"\n",
    "        \n",
    "    def parameters(self):\n",
    "        for layer in model.layers:\n",
    "            for key, value in layer.parameters.items():\n",
    "                yield value\n",
    "        \n",
    "    def add(self, layers):\n",
    "        if isinstance(layers, list):\n",
    "            self.layers += layers\n",
    "        else:\n",
    "            self.layers.append(layers)\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for k, v in l1.__dict__.items():\n",
    "            if isinstance(v, Tensor):\n",
    "                if v.is_param:\n",
    "                    v.zero_grad()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        input_layer = x\n",
    "        for layer in self.layers:\n",
    "            y_pred = layer(input_layer)\n",
    "            input_layer = layer.y\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \n",
    "    def __init__(self, loss, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        \n",
    "    def compute_gradients(self, model, y, y_pred):\n",
    "        \n",
    "        n_layers = len(model.layers)\n",
    "        output_layer = model.layers[-1]\n",
    "        m = model.layers[0].x.shape[0]\n",
    "        # For each output unit, calculate it's error term\n",
    "        delta = self.loss.grad(y, y_pred) * output_layer.activation_fn.grad(output_layer.z)\n",
    "        output_layer.W.grad = (1 / m) * output_layer.x.T @ delta\n",
    "        output_layer.b.grad = (1 / m) * np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        # For hidden units, calculate error term\n",
    "        for i in np.arange(n_layers - 2, -1, -1):\n",
    "            delta = (delta @ model.layers[i+1].W.T) * model.layers[i].activation_fn.grad(model.layers[i].z)\n",
    "            model.layers[i].W.grad = (1 / m) * (model.layers[i].x.T @ delta)\n",
    "            model.layers[i].b.grad = (1 / m) * np.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "    def apply_gradients(self, model):\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            layer.W = layer.W - self.learning_rate * layer.W.grad\n",
    "            layer.b = layer.b - self.learning_rate * layer.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.losses.loss_fns import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = M()\n",
    "l1 = Lay(784, 128)\n",
    "# l2 = Lay(128, 128)\n",
    "l3 = Lay(128, 10)\n",
    "# Add layers\n",
    "model.add([l1, l3])\n",
    "\n",
    "# Tensorize numpy arrays\n",
    "X_train = Tensor(mnist.X_train)\n",
    "y_train = Tensor(mnist.y_train)\n",
    "X_test = Tensor(mnist.X_test)\n",
    "y_test = Tensor(mnist.y_test)\n",
    "batch_size = 64\n",
    "\n",
    "# Define loss\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = SGD(loss, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss=0.2223, train accuracy=0.8673, test accuracy=0.8763\n",
      "Iteration 1: Loss=0.1298, train accuracy=0.9023, test accuracy=0.9068\n",
      "Iteration 2: Loss=0.1223, train accuracy=0.9150, test accuracy=0.9194\n",
      "Iteration 3: Loss=0.1193, train accuracy=0.9247, test accuracy=0.9272\n",
      "Iteration 4: Loss=0.1168, train accuracy=0.9321, test accuracy=0.9321\n",
      "Iteration 5: Loss=0.1145, train accuracy=0.9383, test accuracy=0.9379\n",
      "Iteration 6: Loss=0.1122, train accuracy=0.9430, test accuracy=0.9425\n",
      "Iteration 7: Loss=0.1102, train accuracy=0.9468, test accuracy=0.9457\n",
      "Iteration 8: Loss=0.1084, train accuracy=0.9501, test accuracy=0.9485\n",
      "Iteration 9: Loss=0.1067, train accuracy=0.9536, test accuracy=0.9512\n",
      "Iteration 10: Loss=0.1052, train accuracy=0.9563, test accuracy=0.9530\n",
      "Iteration 11: Loss=0.1038, train accuracy=0.9587, test accuracy=0.9554\n",
      "Iteration 12: Loss=0.1026, train accuracy=0.9606, test accuracy=0.9567\n",
      "Iteration 13: Loss=0.1014, train accuracy=0.9626, test accuracy=0.9583\n",
      "Iteration 14: Loss=0.1004, train accuracy=0.9643, test accuracy=0.9600\n",
      "Iteration 15: Loss=0.0994, train accuracy=0.9665, test accuracy=0.9617\n",
      "Iteration 16: Loss=0.0985, train accuracy=0.9680, test accuracy=0.9633\n",
      "Iteration 17: Loss=0.0977, train accuracy=0.9696, test accuracy=0.9644\n",
      "Iteration 18: Loss=0.0969, train accuracy=0.9709, test accuracy=0.9652\n",
      "Iteration 19: Loss=0.0962, train accuracy=0.9721, test accuracy=0.9662\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in np.arange(epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for start in range(0, X_train.shape[0], batch_size):\n",
    "        end = start + batch_size\n",
    "    \n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        # Feedforward\n",
    "        y_pred = model.forward(X_batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        batch_loss = loss.loss(y_batch, y_pred, reduce=True).data\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.compute_gradients(model, y_batch, y_pred)\n",
    "\n",
    "        # Update gradients\n",
    "        optimizer.apply_gradients(model)\n",
    "        \n",
    "        epoch_loss = epoch_loss + batch_loss\n",
    "        n_batches += 1\n",
    "        \n",
    "    epoch_loss = epoch_loss / n_batches\n",
    "\n",
    "    # Calculate accuracy\n",
    "    y_pred_train = model.forward(X_train)\n",
    "    train_acc = accuracy_score(y_train.data, y_pred_train.data)\n",
    "    # Test accuracy\n",
    "    y_pred_test = model.forward(X_test)\n",
    "    test_acc = accuracy_score(y_test.data, y_pred_test.data)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Iteration {epoch}: Loss={epoch_loss:.4f}, train accuracy={train_acc:.4f}, test accuracy={test_acc:.4f}\")\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.layers import Layer\n",
    "from fault_tolerant_ml import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Layer(784, 32)\n",
    "l2 = Layer(32, 10)\n",
    "model.add(l1)\n",
    "model.add(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(model):\n",
    "    for layer in model.layers:\n",
    "        for k, v in layer.__dict__.items():\n",
    "            if 'W' == k or 'b' == k:\n",
    "                yield v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 4.96714143e-03 -1.38264301e-03  6.47688564e-03 ...  2.19045561e-02\n",
      "  -9.90536343e-03 -5.66297676e-03]\n",
      " [ 9.96513641e-04 -5.03475638e-03 -1.55066345e-02 ...  1.03246523e-02\n",
      "  -1.51936989e-02 -4.84234048e-03]\n",
      " [ 1.26691116e-02 -7.07669416e-03  4.43819445e-03 ... -5.99392643e-03\n",
      "  -2.12389566e-02 -5.25755016e-03]\n",
      " ...\n",
      " [ 6.64494326e-03 -4.93281335e-03  1.12824291e-05 ...  1.63267069e-02\n",
      "   1.80625240e-03 -4.44629369e-03]\n",
      " [-8.25501606e-03  1.19469017e-02 -1.72516075e-03 ...  4.61553223e-03\n",
      "   6.46319659e-03  5.02304174e-03]\n",
      " [-4.25684825e-03 -4.09969967e-03 -1.21127363e-04 ...  1.22946026e-02\n",
      "   1.61097292e-02  1.38859004e-02]], parameter=True)\n",
      "Tensor([[-0.00220694 -0.00320348 -0.00170471 -0.0017872  -0.00213755 -0.00182861\n",
      "  -0.0025094  -0.00156471 -0.0010573  -0.00269023 -0.0025959  -0.00218623\n",
      "  -0.00268075 -0.00257969 -0.00195584 -0.00290803 -0.00217354 -0.00187287\n",
      "  -0.0016702  -0.00224794 -0.0023008  -0.00212161 -0.00115739 -0.00272205\n",
      "  -0.00248753 -0.00159402 -0.00273759 -0.0024076  -0.00237694 -0.00177584\n",
      "  -0.00264389 -0.00315834 -0.00225544 -0.00253386 -0.00174005 -0.00173666\n",
      "  -0.00298528 -0.0013632  -0.00185445 -0.00187044 -0.00246587 -0.00139528\n",
      "  -0.00253635 -0.00188184 -0.00275595 -0.00270564 -0.002977   -0.00237979\n",
      "  -0.0009666  -0.00137753 -0.00268803 -0.00187134 -0.00261864 -0.00174992\n",
      "  -0.00226177 -0.00183054 -0.0026371  -0.00119662 -0.00197094 -0.00247669\n",
      "  -0.00159133 -0.00199855 -0.00263677 -0.00185962 -0.0021234  -0.00281195\n",
      "  -0.00277323 -0.00288541 -0.00270821 -0.00207466 -0.00284128 -0.00246453\n",
      "  -0.00151568 -0.00193894 -0.00291313 -0.00228499 -0.00281456 -0.00272843\n",
      "  -0.00214502 -0.00153215 -0.00207618 -0.00238169 -0.00245287 -0.0022093\n",
      "  -0.00137606 -0.00206751 -0.00244468 -0.00232939 -0.00157172 -0.00137787\n",
      "  -0.00218468 -0.00319848 -0.00163994 -0.0017448  -0.00203049 -0.00222446\n",
      "  -0.00153783 -0.00251694 -0.00288152 -0.00181368 -0.0023199  -0.00266109\n",
      "  -0.00222914 -0.00194905 -0.00271169 -0.00199765 -0.00233847 -0.00250197\n",
      "  -0.00167398 -0.00200826 -0.00229101 -0.00199247 -0.00280799 -0.0010489\n",
      "  -0.00148708 -0.00160287 -0.00321257 -0.00164497 -0.00191243 -0.0018805\n",
      "  -0.00175398 -0.00272185 -0.00280183 -0.00202957 -0.00213861 -0.00144303\n",
      "  -0.00160893 -0.0018609 ]], parameter=True)\n",
      "Tensor([[-0.03759082 -0.02472559 -0.02382299 ... -0.02617643 -0.03894854\n",
      "  -0.04191996]\n",
      " [-0.03706585 -0.03307051 -0.02794755 ... -0.03406617 -0.01895711\n",
      "  -0.0424525 ]\n",
      " [-0.01908701 -0.04578771 -0.03682885 ... -0.02004214 -0.03586865\n",
      "  -0.02463424]\n",
      " ...\n",
      " [-0.04262041 -0.04772541 -0.03818009 ... -0.05212376 -0.03230119\n",
      "  -0.05164885]\n",
      " [-0.02394039 -0.04516073 -0.04956582 ... -0.03250915 -0.04304665\n",
      "  -0.04067078]\n",
      " [-0.04014253 -0.03249479 -0.02696558 ... -0.03883147 -0.03235672\n",
      "  -0.03265183]], parameter=True)\n",
      "Tensor([[-0.0670274  -0.06447035 -0.06660236 -0.06161588 -0.06572702 -0.06902114\n",
      "  -0.06757224 -0.06738191 -0.06643366 -0.06747183]], parameter=True)\n",
      "Tensor([[ 0.01633212  0.00229872 -0.01439324 ...  0.00307093 -0.00297367\n",
      "  -0.00609329]\n",
      " [ 0.01116638  0.01060815 -0.00327281 ... -0.010531    0.0095692\n",
      "   0.01190886]\n",
      " [-0.00498611  0.00637896 -0.00229312 ... -0.0252146  -0.00933481\n",
      "   0.00892555]\n",
      " ...\n",
      " [-0.00241825 -0.02537581  0.00518438 ...  0.00493529 -0.00096637\n",
      "   0.00926067]\n",
      " [ 0.00477757  0.01269783  0.00393316 ...  0.00546764 -0.00168586\n",
      "  -0.00113677]\n",
      " [ 0.00443621 -0.00740898 -0.01519649 ...  0.00505593  0.00084997\n",
      "  -0.00128655]], parameter=True)\n",
      "Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]], parameter=True)\n",
      "Tensor([[-5.44582540e-03  3.45731131e-03  1.34338848e-02 -6.67915260e-03\n",
      "  -6.13517826e-03  1.93883700e-03  1.21284816e-02 -1.71987284e-02\n",
      "   7.80084822e-03 -4.33769729e-03]\n",
      " [ 1.44655155e-02 -1.28847957e-02  1.21690333e-02  1.01655032e-02\n",
      "   2.52910517e-02 -8.82209837e-03  5.45620825e-03  8.36695195e-04\n",
      "  -2.95705139e-03  1.54685241e-03]\n",
      " [ 2.24562380e-02 -3.20288911e-03 -1.75526254e-02 -2.01900443e-03\n",
      "  -6.50134683e-03 -1.14370901e-02 -1.69590916e-02  1.27726197e-02\n",
      "   2.82049965e-04 -8.71339999e-03]\n",
      " [-7.39808148e-03  2.80846898e-02  8.84351600e-03 -6.23523118e-03\n",
      "  -5.75737888e-03  1.96732283e-02  7.83252902e-03 -1.26491934e-02\n",
      "  -9.69093665e-03  9.58714727e-03]\n",
      " [ 8.85924324e-03  6.76353183e-03  1.29716424e-03 -7.17583345e-03\n",
      "   1.45306503e-02  7.24455109e-04  1.29290866e-02  1.13010630e-02\n",
      "   7.08412193e-03 -4.31367487e-04]\n",
      " [-2.35262467e-03 -7.36768066e-04  1.41794449e-02 -4.11242433e-03\n",
      "  -1.03249389e-03  5.63035021e-03  1.88153423e-02  5.23104426e-03\n",
      "   1.63384923e-03 -3.52930534e-03]\n",
      " [ 2.55777687e-02 -1.51719223e-03 -9.24489088e-03  9.85499751e-03\n",
      "   4.86668060e-03 -7.18525564e-03  1.63294084e-03  6.58311788e-03\n",
      "   8.08919035e-03 -1.90407492e-03]\n",
      " [ 5.96441654e-03  1.74023416e-02  3.53396917e-03  8.70192330e-03\n",
      "  -2.05216813e-03 -3.51657625e-03 -2.92958948e-03 -1.02911098e-02\n",
      "  -1.49419568e-02  8.16295203e-03]\n",
      " [ 6.40687440e-03  3.20625468e-03 -1.18548907e-02  2.24830415e-02\n",
      "  -1.22007988e-02 -6.15666388e-03 -4.93742619e-03 -4.33701975e-03\n",
      "   9.30255838e-03 -9.37122386e-04]\n",
      " [-1.02366675e-02  6.45202305e-03 -6.77007996e-03 -5.20662963e-03\n",
      "  -1.35974949e-02  7.31078116e-03 -1.06743677e-02 -1.33002577e-02\n",
      "   1.49457725e-02  1.31411450e-02]\n",
      " [ 3.33848060e-03 -1.14677502e-02  7.10296025e-03 -6.61412720e-03\n",
      "  -4.30241786e-03  3.68791074e-03  2.19546296e-02  1.17586125e-02\n",
      "   1.48691237e-02 -3.39679164e-03]\n",
      " [ 2.43387907e-03  1.17267547e-02  7.39082228e-04 -1.60959773e-02\n",
      "  -1.72976237e-02 -8.34433828e-03  6.94738980e-03 -9.08957422e-03\n",
      "   8.32047313e-03  1.58411842e-02]\n",
      " [-1.47464285e-02  2.30673794e-02  3.44187859e-03 -9.03991330e-03\n",
      "   1.07251955e-02 -5.84501354e-03  2.48731915e-02 -6.31266227e-03\n",
      "  -5.72043099e-03  9.92001407e-03]\n",
      " [ 5.67209115e-03 -1.08133527e-02 -6.55752840e-04  4.82794177e-03\n",
      "  -7.46033678e-04  2.24293210e-02 -7.50031741e-03  5.78469550e-03\n",
      "   2.66060028e-02 -5.65851456e-04]\n",
      " [-8.70862883e-03  4.59610904e-03 -1.09474929e-02 -1.07015930e-02\n",
      "  -1.83642693e-02 -2.50556991e-02  1.16539951e-02  1.95647217e-03\n",
      "  -2.00876687e-03  1.38289866e-03]\n",
      " [-1.76135469e-02  9.19342693e-03  6.24971092e-03  1.09429415e-02\n",
      "  -6.11048983e-03 -6.54103095e-03  6.11603167e-03  1.10485489e-02\n",
      "   6.00618543e-03 -2.10235268e-03]\n",
      " [-8.51958059e-03 -2.42264383e-03 -6.75682398e-03 -8.09783489e-03\n",
      "  -4.80080227e-04  6.28523575e-03 -9.06480290e-03  3.81561508e-03\n",
      "   1.16006220e-02  6.50909147e-04]\n",
      " [-1.82814077e-02 -1.80208720e-02 -2.47817906e-03  9.35068540e-03\n",
      "  -4.08557756e-03  5.07326936e-03 -2.67706066e-03  5.54465177e-03\n",
      "  -1.30878540e-03 -7.64565601e-04]\n",
      " [ 2.57713031e-02  6.79030037e-03  6.66057318e-03 -4.06108471e-03\n",
      "   1.96610764e-03 -9.70402325e-04  4.85893665e-03 -4.79592895e-03\n",
      "  -9.03929025e-03  2.36819666e-02]\n",
      " [ 2.93294992e-03 -6.69681001e-03 -6.93699578e-03 -6.15594070e-03\n",
      "  -7.83747528e-03 -8.54472071e-03 -1.95953151e-04  1.43562807e-02\n",
      "  -1.89577341e-02 -7.31013762e-03]\n",
      " [-7.95909017e-03 -3.41455336e-03 -5.69773838e-03  1.34554587e-03\n",
      "  -5.12329349e-03  3.50143551e-03  3.67946806e-03  2.99634831e-03\n",
      "   3.02511081e-03 -2.20450256e-02]\n",
      " [-6.20129285e-05 -1.50086973e-02 -7.58295180e-03  6.59936282e-04\n",
      "   2.61317194e-03 -1.18835352e-03 -8.79182480e-03  9.54522868e-04\n",
      "   1.98995117e-02  6.67342031e-03]\n",
      " [ 1.34324562e-02 -3.82140279e-03 -8.09965748e-03  8.12119432e-03\n",
      "   2.49414220e-02 -7.38610420e-03 -5.27674472e-03  1.35394214e-02\n",
      "  -1.21712694e-02 -1.55082764e-03]\n",
      " [ 1.60552617e-02  1.38717867e-03 -1.83838140e-02 -1.32326968e-02\n",
      "  -1.62711609e-02  7.16996659e-03 -7.37588108e-03  1.16949342e-02\n",
      "  -4.60322713e-03  1.83679536e-03]\n",
      " [-1.14923716e-02  6.71218615e-04  2.95195612e-03 -2.96080858e-03\n",
      "   1.13488855e-02 -5.61013026e-03 -6.35417271e-03 -1.14345900e-03\n",
      "   1.30203888e-02 -5.40898135e-03]\n",
      " [ 3.16325831e-03  7.27556506e-03  1.54751353e-02 -1.07005211e-02\n",
      "  -4.09070216e-03  1.13529144e-02 -8.11280310e-03  1.26546295e-02\n",
      "   1.57281663e-02  1.14821456e-02]\n",
      " [-5.53778699e-03  1.87024882e-03  5.18597895e-03  5.41601330e-03\n",
      "  -4.97428467e-03  6.16343925e-03  1.02722999e-02  1.09004518e-02\n",
      "   4.52385604e-04 -1.92038324e-02]\n",
      " [-1.58976601e-03 -1.39860008e-02  4.87679522e-03 -5.71486540e-03\n",
      "  -2.59839417e-03 -5.64993499e-03  1.61577407e-02  8.75267945e-03\n",
      "  -1.47513533e-03  1.16949119e-02]\n",
      " [ 7.74482591e-03  9.69161093e-03 -2.43194066e-02  2.55510751e-02\n",
      "   1.22735463e-02 -1.36342586e-03  9.20648035e-03 -1.34541746e-03\n",
      "   1.15630161e-02  4.15302860e-03]\n",
      " [-1.48666846e-02 -5.01372525e-03  1.56002725e-02 -9.36112180e-03\n",
      "   1.95930898e-03  1.50889605e-02  1.18129952e-02 -3.77701363e-03\n",
      "  -4.21061926e-03 -1.31511241e-02]\n",
      " [ 8.22781213e-03 -2.47306260e-03  2.64335144e-03  2.69115102e-02\n",
      "   1.45761983e-03  7.00689061e-03  8.97594448e-03 -4.58152778e-03\n",
      "  -2.26579187e-03 -9.44717042e-03]\n",
      " [-7.64714275e-03  1.33518100e-04 -1.93181692e-03 -8.10513645e-03\n",
      "  -3.62615520e-03 -1.05291540e-02 -4.74900845e-03  9.46062990e-03\n",
      "   6.04748121e-03 -2.12542596e-03]], parameter=True)\n",
      "Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], parameter=True)\n"
     ]
    }
   ],
   "source": [
    "for param in parameters(model):\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ftml]",
   "language": "python",
   "name": "conda-env-ftml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
