{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from fault_tolerant_ml.data.mnist import MNist\n",
    "import fault_tolerant_ml.activations.activation as F\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "filepaths = {\n",
    "    \"train\": {\n",
    "        \"images\": os.path.join(data_dir, \"train-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"train-labels-idx1-ubyte.gz\")\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"images\": os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"t10k-labels-idx1-ubyte.gz\")\n",
    "    }\n",
    "}\n",
    "mnist = MNist(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/fashion-mnist/\"\n",
    "filepaths = {\n",
    "    \"train\": {\n",
    "        \"images\": os.path.join(data_dir, \"train-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"train-labels-idx1-ubyte.gz\")\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"images\": os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"t10k-labels-idx1-ubyte.gz\")\n",
    "    }\n",
    "}\n",
    "fashion_mnist = MNist(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MNist X_train=(60000, 784), y_train=(60000, 10), X_test=(10000, 784), y_test=(10000, 10)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features=784, n_classes=10\n"
     ]
    }
   ],
   "source": [
    "n_features, n_classes = mnist.X_train.shape[1], mnist.y_train.shape[1]\n",
    "print(f\"n_features={n_features}, n_classes={n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNet(nn.Model):\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         # MLP - 1 input layer, 1 hidden layer, 1 output layer\n",
    "#         # self.fc1 = nn.Layer(n_inputs=784, n_outputs=128)\n",
    "#         # self.fc2 = nn.Layer(n_inputs=128, n_outputs=10)\n",
    "#         self.layers = []\n",
    "#         # self.act_fn = F.Sigmoid()\n",
    "        \n",
    "#     def add(self, layer):\n",
    "#         \"\"\"Add layer to model\n",
    "#         \"\"\"\n",
    "#         self.layers.append(layer)\n",
    "#         return self\n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # z1 = self.fc1(x)\n",
    "#         # a1 = self.act_fn(z1)\n",
    "#         # a1, z1 = self.fc1(x)\n",
    "#         # z2 = self.fc2(a1)\n",
    "#         # y_pred = self.act_fn(z2)\n",
    "#         # y_pred, z2 = self.fc2(a1)\n",
    "        \n",
    "#         a = x\n",
    "#         for layer in self.layers:\n",
    "#             print(layer)\n",
    "#             layer(a)\n",
    "#             a = layer.y\n",
    "        \n",
    "#         y_pred = a\n",
    "#         return y_pred\n",
    "    \n",
    "#     def backward():\n",
    "#         pass\n",
    "    \n",
    "# #     def backward(self, x, y, a_n):\n",
    "        \n",
    "# #         y_pred, z2, a1, z1 = a_n\n",
    "# #         # Output layer error\n",
    "# #         delta2 = (y_pred - y)# * self.act_fn.grad(z2)\n",
    "# #         # Gradient of cost function\n",
    "# #         dw2 = np.dot(a1.T, delta2)\n",
    "# #         # Backpropagate the error through the network\n",
    "# #         delta1 = np.dot(delta2, self.fc2.W.T) * self.act_fn.grad(z1)\n",
    "# #         # Calculate gradient\n",
    "# #         dw1 = np.dot(x.T, delta1)\n",
    "# #         # Gradient of biases equal to the error\n",
    "# #         db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "# #         db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "# #         return dw2, db2, dw1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy_loss(y_pred, y):\n",
    "#     return np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy_score(y, y_pred):\n",
    "#     y_pred_ = y_pred.argmax(axis=1)\n",
    "#     y_ = y.argmax(axis=1)\n",
    "#     return np.sum(y_pred_==y_) / y_.shape[0]\n",
    "\n",
    "# from fault_tolerant_ml.ml.ops.tensor import Tensor\n",
    "\n",
    "# t = Tensor(mnist.X_train)\n",
    "# W = Tensor(np.random.randn(784, 128))\n",
    "# b = Tensor(np.random.randn(1, 128))\n",
    "\n",
    "# x1 = Tensor(np.array(3), requires_grad=True)\n",
    "# x2 = Tensor(np.array(7), requires_grad=False)\n",
    "\n",
    "# interim = ((x1 + x1) + x2)\n",
    "# f = interim * interim\n",
    "\n",
    "# $(2x_1 + x_2)^2 = 4x_1^2 + 4x_1x_2 +x_2^2$ \n",
    "\n",
    "# $\\frac{\\partial{df}}{\\partial{x_1}} =8x_1 + 4x_2 = 24 + 28 = 52$\n",
    "\n",
    "# $\\frac{\\partial{df}}{\\partial{x_2}} =4x_1 + 2x_2 = 12 + 14 = 26$\n",
    "\n",
    "# f.backward()\n",
    "\n",
    "# x1.grad\n",
    "\n",
    "# model = NeuralNet()\n",
    "# model.add(nn.Layer(n_inputs=784, n_outputs=128))\n",
    "# model.add(nn.Layer(n_inputs=128, n_outputs=10))\n",
    "# y_pred = model.forward(mnist.X_train)\n",
    "\n",
    "# l = \n",
    "# for layer in model.layers[::-1]:\n",
    "#     print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNet()\n",
    "# print(model.fc1.shape)\n",
    "# print(model.fc2.shape)\n",
    "# epochs = 400\n",
    "# learning_rate = 0.99\n",
    "# m = mnist.X_train.shape[0]\n",
    "# for epoch in np.arange(epochs):\n",
    "    \n",
    "#     # Feedforward\n",
    "#     y_pred, z2, a1, z1 = model.forward(mnist.X_train)\n",
    "    \n",
    "#     # Calculate cost\n",
    "#     loss = cross_entropy_loss(y_pred, mnist.y_train)\n",
    "    \n",
    "#     # Backprop\n",
    "#     dw2, db2, dw1, db1 = model.backward(mnist.X_train, mnist.y_train, [y_pred, z2, a1, z1])\n",
    "    \n",
    "#     # Update weights\n",
    "#     model.fc2.W = model.fc2.W - learning_rate * 1 / m * dw2\n",
    "#     model.fc1.W = model.fc1.W - learning_rate * 1 / m * dw1\n",
    "#     model.fc2.b = model.fc2.b - learning_rate * 1 / m * db2\n",
    "#     model.fc1.b = model.fc1.b - learning_rate * 1 / m * db1\n",
    "    \n",
    "#     acc = accuracy_score(mnist.y_train, y_pred)\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'epoch = {epoch}, loss = {loss:.3f}, TRAIN ACC = {acc:.3f}')\n",
    "#     epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = Graph()\n",
    "\n",
    "# g.set_as_default()\n",
    "\n",
    "# X = Tensor(mnist.X_train)\n",
    "\n",
    "# X + W\n",
    "\n",
    "# W = Variable(np.random.rand(784, 128))\n",
    "\n",
    "# b = Variable(np.zeros(shape=(1, 128)))\n",
    "\n",
    "# z = add(matmul(X, W), b)\n",
    "\n",
    "# z.input_nodes[0].input_nodes[0]\n",
    "\n",
    "# z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fault_tolerant_ml.ml.ops import tensor as ft\n",
    "\n",
    "# g = ft.Graph()\n",
    "# g.set_as_default()\n",
    "# X = ft.Tensor(mnist.X_train)\n",
    "# y = ft.Tensor(mnist.y_train)\n",
    "\n",
    "# W = ft.Tensor(np.random.randn(784, 128))\n",
    "\n",
    "# b = ft.Tensor(np.zeros((1, 784)))\n",
    "\n",
    "# a = ft.matmul(X, W)\n",
    "# # z = ft.add(W, b)\n",
    "\n",
    "# g.operations\n",
    "\n",
    "# def evalulate(f):\n",
    "#     val = []\n",
    "#     for i, op in enumerate(f.operations):\n",
    "#         print(*op.input_nodes)\n",
    "#         val.append(op.compute(*op.input_nodes))\n",
    "\n",
    "# def traverse(f):\n",
    "    \n",
    "#     operations = []\n",
    "#     def recurse(node):\n",
    "#         if isinstance(node, ft.Operation):\n",
    "#             for input_node in node.input_nodes:\n",
    "#                 recurse(input_node)\n",
    "#         operations.append(node)\n",
    "            \n",
    "#     recurse(f)\n",
    "#     return operations\n",
    "\n",
    "# g = ft.Graph()\n",
    "# g.set_as_default()\n",
    "# x1 = ft.Tensor(np.array(3))\n",
    "# x2 = ft.Tensor(np.array(7))\n",
    "\n",
    "# f = ft.square(ft.add(ft.add(x1, x1), x2))\n",
    "\n",
    "# traverse(f)\n",
    "\n",
    "# g.operations[0].input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalulate(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [ (\"z1\", \"add\", (\"x1\",\"x1\")),\n",
    "# (\"z2\", \"add\", (\"z1\",\"x2\")),\n",
    "# (\"f\", \"square\", (\"z2\",)) ]\n",
    "\n",
    "# G = { \"add\" : lambda a,b: a+b,\n",
    "# \"square\": lambda a:a*a }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = { \"x1\" : 3, \"x2\" : 7 }\n",
    "\n",
    "# for step in l:\n",
    "#     print(val)\n",
    "#     var, op_name, func = step\n",
    "#     lookup = list(map(val.get, func))\n",
    "#     val[var] = G[op_name](*lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DG = { \"add\" : [ (lambda a,b: 1), (lambda a,b: 1) ],\n",
    "# \"square\": [ lambda a:2*a ] }\n",
    "\n",
    "# delta={}\n",
    "# delta[\"f\"] = 1\n",
    "# for step in l[::-1]:\n",
    "#     var, op_name, func = step\n",
    "#     for op in DG[op_name]:\n",
    "#         if var not in delta:\n",
    "#             delta[var] = 0\n",
    "#         lookup = list(map(val.get, func))\n",
    "#         print(lookup)\n",
    "#         delta[var] = delta[var] + DG[op_name](*lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tensor(object):\n",
    "    \n",
    "#     def __init__(self, data: np.ndarray, depends_on=None):\n",
    "        \n",
    "#         self.depends_on = depends_on or []\n",
    "#         self.data = data\n",
    "            \n",
    "#     def __add__(self, other):\n",
    "#         return Tensor(self.data + other.data, depends_on=[self, other])\n",
    "    \n",
    "#     def __pow__(self, p):\n",
    "#         data = self.data ** p \n",
    "#         return Tensor(data, depends_on=[self])\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f\"Tensor({self.data}, dtype={self.data.dtype})\"\n",
    "\n",
    "\n",
    "# y1 = Tensor(np.array(3))\n",
    "# y2 = Tensor(np.array(7))\n",
    "# z1 = y1 + y1\n",
    "# z2 = z1 ** 2\n",
    "\n",
    "# z2.depends_on\n",
    "\n",
    "# layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.activations.activation import Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_tolerant_ml.losses.loss_fns import CrossEntropyLoss, MSELoss\n",
    "from fault_tolerant_ml.models import Model\n",
    "from fault_tolerant_ml.layers import Layer\n",
    "from fault_tolerant_ml.optimizers import SGD\n",
    "from fault_tolerant_ml.metrics import accuracy_scorev2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=100):\n",
    "    for epoch in np.arange(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for start in range(0, X_train.shape[0], batch_size):\n",
    "            end = start + batch_size\n",
    "\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Feedforward\n",
    "            y_pred = model.forward(X_batch)\n",
    "            \n",
    "            print(y_pred[1])\n",
    "\n",
    "            # Calculate loss\n",
    "            batch_loss = loss.loss(y_batch, y_pred, reduce=True).data\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.compute_gradients(model, y_batch, y_pred)\n",
    "\n",
    "            # Update gradients\n",
    "            optimizer.apply_gradients(model)\n",
    "\n",
    "            epoch_loss = epoch_loss + batch_loss\n",
    "            n_batches += 1\n",
    "\n",
    "        epoch_loss = epoch_loss / n_batches\n",
    "\n",
    "        # Calculate accuracy\n",
    "        y_pred_train = model.forward(X_train)\n",
    "        train_acc = accuracy_scorev2(y_train.data, y_pred_train.data)\n",
    "        # Test accuracy\n",
    "        y_pred_test = model.forward(X_test)\n",
    "        test_acc = accuracy_scorev2(y_test.data, y_pred_test.data)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Iteration {epoch}: Loss={epoch_loss:.4f}, train accuracy={train_acc:.4f}, test accuracy={test_acc:.4f}\")\n",
    "        epoch += 1\n",
    "        \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "# loss = CrossEntropyLoss()\n",
    "loss = MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = SGD(loss, learning_rate=0.001)\n",
    "\n",
    "# Define model\n",
    "model = Model(optimizer=optimizer)\n",
    "l1 = Layer(784, 10, activation=\"linear\")\n",
    "# l2 = Lay(128, 128)\n",
    "# l3 = Layer(128, 10)\n",
    "# Add layers\n",
    "model.add([l1])\n",
    "\n",
    "# Tensorize numpy arrays\n",
    "X_train = mnist.X_train\n",
    "y_train = mnist.y_train\n",
    "X_test = mnist.X_test\n",
    "y_test = mnist.y_test\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-afe55843e4c4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, X_test, y_test, batch_size, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/fault-tolerant-ml/fault_tolerant_ml/operators/tensorpy.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mis_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model, X_train, y_train, X_test, y_test, batch_size=batch_size, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loss = MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "f_optimizer = SGD(f_loss, learning_rate=0.001)\n",
    "\n",
    "# Define model\n",
    "f_model = Model(optimizer=optimizer)\n",
    "f_l1 = Layer(784, 10, activation=\"linear\")\n",
    "# l2 = Lay(128, 128)\n",
    "# l3 = Layer(128, 10)\n",
    "# Add layers\n",
    "f_model.add([f_l1])\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss=0.0258, train accuracy=0.7375, test accuracy=0.7279\n",
      "Iteration 1: Loss=0.0210, train accuracy=0.7761, test accuracy=0.7645\n",
      "Iteration 2: Loss=0.0198, train accuracy=0.7904, test accuracy=0.7785\n",
      "Iteration 3: Loss=0.0192, train accuracy=0.7976, test accuracy=0.7860\n",
      "Iteration 4: Loss=0.0188, train accuracy=0.8028, test accuracy=0.7913\n",
      "Iteration 5: Loss=0.0185, train accuracy=0.8060, test accuracy=0.7947\n",
      "Iteration 6: Loss=0.0183, train accuracy=0.8088, test accuracy=0.7982\n",
      "Iteration 7: Loss=0.0181, train accuracy=0.8111, test accuracy=0.8006\n",
      "Iteration 8: Loss=0.0180, train accuracy=0.8127, test accuracy=0.8016\n",
      "Iteration 9: Loss=0.0179, train accuracy=0.8135, test accuracy=0.8035\n",
      "Iteration 10: Loss=0.0178, train accuracy=0.8146, test accuracy=0.8047\n",
      "Iteration 11: Loss=0.0177, train accuracy=0.8155, test accuracy=0.8048\n",
      "Iteration 12: Loss=0.0176, train accuracy=0.8161, test accuracy=0.8055\n",
      "Iteration 13: Loss=0.0176, train accuracy=0.8169, test accuracy=0.8060\n",
      "Iteration 14: Loss=0.0175, train accuracy=0.8174, test accuracy=0.8068\n",
      "Iteration 15: Loss=0.0175, train accuracy=0.8182, test accuracy=0.8069\n",
      "Iteration 16: Loss=0.0175, train accuracy=0.8184, test accuracy=0.8069\n",
      "Iteration 17: Loss=0.0174, train accuracy=0.8189, test accuracy=0.8070\n",
      "Iteration 18: Loss=0.0174, train accuracy=0.8192, test accuracy=0.8074\n",
      "Iteration 19: Loss=0.0174, train accuracy=0.8194, test accuracy=0.8078\n",
      "Iteration 20: Loss=0.0173, train accuracy=0.8198, test accuracy=0.8083\n",
      "Iteration 21: Loss=0.0173, train accuracy=0.8201, test accuracy=0.8087\n",
      "Iteration 22: Loss=0.0173, train accuracy=0.8203, test accuracy=0.8089\n",
      "Iteration 23: Loss=0.0173, train accuracy=0.8206, test accuracy=0.8092\n",
      "Iteration 24: Loss=0.0173, train accuracy=0.8209, test accuracy=0.8095\n",
      "Iteration 25: Loss=0.0172, train accuracy=0.8210, test accuracy=0.8098\n",
      "Iteration 26: Loss=0.0172, train accuracy=0.8212, test accuracy=0.8101\n",
      "Iteration 27: Loss=0.0172, train accuracy=0.8215, test accuracy=0.8108\n",
      "Iteration 28: Loss=0.0172, train accuracy=0.8214, test accuracy=0.8112\n",
      "Iteration 29: Loss=0.0172, train accuracy=0.8217, test accuracy=0.8113\n",
      "Iteration 30: Loss=0.0172, train accuracy=0.8218, test accuracy=0.8113\n",
      "Iteration 31: Loss=0.0172, train accuracy=0.8220, test accuracy=0.8110\n",
      "Iteration 32: Loss=0.0172, train accuracy=0.8222, test accuracy=0.8112\n",
      "Iteration 33: Loss=0.0171, train accuracy=0.8225, test accuracy=0.8111\n",
      "Iteration 34: Loss=0.0171, train accuracy=0.8226, test accuracy=0.8115\n",
      "Iteration 35: Loss=0.0171, train accuracy=0.8227, test accuracy=0.8116\n",
      "Iteration 36: Loss=0.0171, train accuracy=0.8227, test accuracy=0.8113\n",
      "Iteration 37: Loss=0.0171, train accuracy=0.8229, test accuracy=0.8113\n",
      "Iteration 38: Loss=0.0171, train accuracy=0.8230, test accuracy=0.8114\n",
      "Iteration 39: Loss=0.0171, train accuracy=0.8231, test accuracy=0.8116\n",
      "Iteration 40: Loss=0.0171, train accuracy=0.8233, test accuracy=0.8117\n",
      "Iteration 41: Loss=0.0171, train accuracy=0.8235, test accuracy=0.8120\n",
      "Iteration 42: Loss=0.0171, train accuracy=0.8238, test accuracy=0.8121\n",
      "Iteration 43: Loss=0.0171, train accuracy=0.8238, test accuracy=0.8125\n",
      "Iteration 44: Loss=0.0171, train accuracy=0.8240, test accuracy=0.8126\n",
      "Iteration 45: Loss=0.0171, train accuracy=0.8241, test accuracy=0.8126\n",
      "Iteration 46: Loss=0.0170, train accuracy=0.8241, test accuracy=0.8125\n",
      "Iteration 47: Loss=0.0170, train accuracy=0.8241, test accuracy=0.8126\n",
      "Iteration 48: Loss=0.0170, train accuracy=0.8242, test accuracy=0.8127\n",
      "Iteration 49: Loss=0.0170, train accuracy=0.8243, test accuracy=0.8129\n",
      "Iteration 50: Loss=0.0170, train accuracy=0.8244, test accuracy=0.8129\n",
      "Iteration 51: Loss=0.0170, train accuracy=0.8244, test accuracy=0.8129\n",
      "Iteration 52: Loss=0.0170, train accuracy=0.8245, test accuracy=0.8130\n",
      "Iteration 53: Loss=0.0170, train accuracy=0.8246, test accuracy=0.8129\n",
      "Iteration 54: Loss=0.0170, train accuracy=0.8247, test accuracy=0.8131\n",
      "Iteration 55: Loss=0.0170, train accuracy=0.8247, test accuracy=0.8131\n",
      "Iteration 56: Loss=0.0170, train accuracy=0.8248, test accuracy=0.8131\n",
      "Iteration 57: Loss=0.0170, train accuracy=0.8248, test accuracy=0.8131\n",
      "Iteration 58: Loss=0.0170, train accuracy=0.8249, test accuracy=0.8132\n",
      "Iteration 59: Loss=0.0170, train accuracy=0.8249, test accuracy=0.8132\n",
      "Iteration 60: Loss=0.0170, train accuracy=0.8250, test accuracy=0.8131\n",
      "Iteration 61: Loss=0.0170, train accuracy=0.8250, test accuracy=0.8131\n",
      "Iteration 62: Loss=0.0170, train accuracy=0.8252, test accuracy=0.8131\n",
      "Iteration 63: Loss=0.0170, train accuracy=0.8253, test accuracy=0.8132\n",
      "Iteration 64: Loss=0.0170, train accuracy=0.8254, test accuracy=0.8130\n",
      "Iteration 65: Loss=0.0170, train accuracy=0.8255, test accuracy=0.8128\n",
      "Iteration 66: Loss=0.0170, train accuracy=0.8256, test accuracy=0.8126\n",
      "Iteration 67: Loss=0.0170, train accuracy=0.8256, test accuracy=0.8125\n",
      "Iteration 68: Loss=0.0170, train accuracy=0.8257, test accuracy=0.8126\n",
      "Iteration 69: Loss=0.0170, train accuracy=0.8258, test accuracy=0.8126\n",
      "Iteration 70: Loss=0.0170, train accuracy=0.8259, test accuracy=0.8127\n",
      "Iteration 71: Loss=0.0170, train accuracy=0.8259, test accuracy=0.8125\n",
      "Iteration 72: Loss=0.0169, train accuracy=0.8259, test accuracy=0.8124\n",
      "Iteration 73: Loss=0.0169, train accuracy=0.8259, test accuracy=0.8124\n",
      "Iteration 74: Loss=0.0169, train accuracy=0.8259, test accuracy=0.8124\n",
      "Iteration 75: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 76: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 77: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 78: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 79: Loss=0.0169, train accuracy=0.8261, test accuracy=0.8127\n",
      "Iteration 80: Loss=0.0169, train accuracy=0.8261, test accuracy=0.8126\n",
      "Iteration 81: Loss=0.0169, train accuracy=0.8262, test accuracy=0.8127\n",
      "Iteration 82: Loss=0.0169, train accuracy=0.8263, test accuracy=0.8130\n",
      "Iteration 83: Loss=0.0169, train accuracy=0.8262, test accuracy=0.8129\n",
      "Iteration 84: Loss=0.0169, train accuracy=0.8263, test accuracy=0.8129\n",
      "Iteration 85: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8128\n",
      "Iteration 86: Loss=0.0169, train accuracy=0.8263, test accuracy=0.8127\n",
      "Iteration 87: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8127\n",
      "Iteration 88: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 89: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8125\n",
      "Iteration 90: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 91: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 92: Loss=0.0169, train accuracy=0.8265, test accuracy=0.8126\n",
      "Iteration 93: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8124\n",
      "Iteration 94: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 95: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8125\n",
      "Iteration 96: Loss=0.0169, train accuracy=0.8265, test accuracy=0.8126\n",
      "Iteration 97: Loss=0.0169, train accuracy=0.8266, test accuracy=0.8127\n",
      "Iteration 98: Loss=0.0169, train accuracy=0.8266, test accuracy=0.8126\n",
      "Iteration 99: Loss=0.0169, train accuracy=0.8267, test accuracy=0.8127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01689868272691965"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(\n",
    "    f_model,\n",
    "    fashion_mnist.X_train,\n",
    "    fashion_mnist.y_train,\n",
    "    fashion_mnist.X_test,\n",
    "    fashion_mnist.y_test,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ftml]",
   "language": "python",
   "name": "conda-env-ftml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
