{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NUM_CORES\"] = \"6\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]= os.environ[\"NUM_CORES\"]\n",
    "os.environ[\"OMP_NUM_THREADS\"]= os.environ[\"NUM_CORES\"]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dyn_fed.data.mnist import MNist\n",
    "import dyn_fed as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = df.data.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_home = Path('/Users/sashlinreddy/.torch/datasets/').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/sashlinreddy/.torch/datasets')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(\n",
    "    torch_home,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = torchvision.datasets.MNIST(\n",
    "    torch_home,\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (fc3): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(hidden_sizes[1], output_size),\n",
    "#                       nn.LogSoftmax(dim=1))\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(input_size, output_size),\n",
    "#     nn.Sigmoid(),\n",
    "# )\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(1, 6, kernel_size=3),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(6, 12, kernel_size=5),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(12 * 4 * 4, 120),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(120, 60),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(60, 10)\n",
    "\n",
    "# )\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(12 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(-1, 12 * 4 * 4)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = SimpleCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777f5c015d324ac7b6726d764d9e1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa0d60b4cd349b9a78ba2a0931bde4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.493, test_loss=0.2687416970729828, train_acc=0.8470166666666666, test_acc=0.9591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0544f023274b9588d6829d56de42fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.091, test_loss=0.08455823361873627, train_acc=0.9708666666666667, test_acc=0.9788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278396bda9e74bf5a26a4f1545badd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.062, test_loss=0.006768633611500263, train_acc=0.9808333333333333, test_acc=0.985\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccf25b6c12c42b4a69dde2108a51dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.047, test_loss=0.004045563284307718, train_acc=0.9853166666666666, test_acc=0.9857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a31e70fd1c466c8004c75d2983c4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.038, test_loss=0.006270758341997862, train_acc=0.9878, test_acc=0.9848\n",
      "\n",
      "\n",
      "Training Time (in minutes) = 1.4607041835784913\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "epochs = 5\n",
    "for e in tqdm(range(epochs)):\n",
    "    running_loss = 0\n",
    "    epoch_losses = []\n",
    "    train_corr = 0\n",
    "    train_total = 0\n",
    "    for images, labels in tqdm(trainloader, total=n_batches):\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        # images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        _, y_pred = torch.max(output.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_corr += (y_pred == labels).sum().item()\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        # running_loss += loss.item()\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "    train_acc = train_corr / train_total\n",
    "        \n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in valloader:\n",
    "            y_pred_val = model(x_val)\n",
    "            batch_test_loss = criterion(output, labels)\n",
    "            test_losses.append(batch_test_loss.numpy())\n",
    "            _, predicted = torch.max(y_pred_val.data, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "            \n",
    "        test_loss = np.mean(test_losses)\n",
    "        test_acc = correct / total\n",
    "        \n",
    "    print(\n",
    "        f\"Epoch {e} - Training loss: {epoch_loss:.3f}, test_loss={test_loss:.3f}, \"\n",
    "        f\"train_acc={train_acc:.3f}, test_acc={test_acc:.3f}\"\n",
    "    )\n",
    "    \n",
    "print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdataset = iter(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.870 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %.3f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ftml]",
   "language": "python",
   "name": "conda-env-ftml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
