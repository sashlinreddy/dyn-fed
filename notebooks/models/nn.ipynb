{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dyn_fed.data.mnist import MNist\n",
    "import dyn_fed.activations.activation as F\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/mnist\"\n",
    "filepaths = {\n",
    "    \"train\": {\n",
    "        \"images\": os.path.join(data_dir, \"train-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"train-labels-idx1-ubyte.gz\")\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"images\": os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"t10k-labels-idx1-ubyte.gz\")\n",
    "    }\n",
    "}\n",
    "mnist = MNist(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/fashion-mnist/\"\n",
    "filepaths = {\n",
    "    \"train\": {\n",
    "        \"images\": os.path.join(data_dir, \"train-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"train-labels-idx1-ubyte.gz\")\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"images\": os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"labels\": os.path.join(data_dir, \"t10k-labels-idx1-ubyte.gz\")\n",
    "    }\n",
    "}\n",
    "fashion_mnist = MNist(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MNist X_train=(60000, 784), y_train=(60000, 10), X_test=(10000, 784), y_test=(10000, 10)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features=784, n_classes=10\n"
     ]
    }
   ],
   "source": [
    "n_features, n_classes = mnist.X_train.shape[1], mnist.y_train.shape[1]\n",
    "print(f\"n_features={n_features}, n_classes={n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNet(nn.Model):\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         # MLP - 1 input layer, 1 hidden layer, 1 output layer\n",
    "#         # self.fc1 = nn.Layer(n_inputs=784, n_outputs=128)\n",
    "#         # self.fc2 = nn.Layer(n_inputs=128, n_outputs=10)\n",
    "#         self.layers = []\n",
    "#         # self.act_fn = F.Sigmoid()\n",
    "        \n",
    "#     def add(self, layer):\n",
    "#         \"\"\"Add layer to model\n",
    "#         \"\"\"\n",
    "#         self.layers.append(layer)\n",
    "#         return self\n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # z1 = self.fc1(x)\n",
    "#         # a1 = self.act_fn(z1)\n",
    "#         # a1, z1 = self.fc1(x)\n",
    "#         # z2 = self.fc2(a1)\n",
    "#         # y_pred = self.act_fn(z2)\n",
    "#         # y_pred, z2 = self.fc2(a1)\n",
    "        \n",
    "#         a = x\n",
    "#         for layer in self.layers:\n",
    "#             print(layer)\n",
    "#             layer(a)\n",
    "#             a = layer.y\n",
    "        \n",
    "#         y_pred = a\n",
    "#         return y_pred\n",
    "    \n",
    "#     def backward():\n",
    "#         pass\n",
    "    \n",
    "# #     def backward(self, x, y, a_n):\n",
    "        \n",
    "# #         y_pred, z2, a1, z1 = a_n\n",
    "# #         # Output layer error\n",
    "# #         delta2 = (y_pred - y)# * self.act_fn.grad(z2)\n",
    "# #         # Gradient of cost function\n",
    "# #         dw2 = np.dot(a1.T, delta2)\n",
    "# #         # Backpropagate the error through the network\n",
    "# #         delta1 = np.dot(delta2, self.fc2.W.T) * self.act_fn.grad(z1)\n",
    "# #         # Calculate gradient\n",
    "# #         dw1 = np.dot(x.T, delta1)\n",
    "# #         # Gradient of biases equal to the error\n",
    "# #         db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "# #         db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "# #         return dw2, db2, dw1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy_loss(y_pred, y):\n",
    "#     return np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy_score(y, y_pred):\n",
    "#     y_pred_ = y_pred.argmax(axis=1)\n",
    "#     y_ = y.argmax(axis=1)\n",
    "#     return np.sum(y_pred_==y_) / y_.shape[0]\n",
    "\n",
    "# from fault_tolerant_ml.ml.ops.tensor import Tensor\n",
    "\n",
    "# t = Tensor(mnist.X_train)\n",
    "# W = Tensor(np.random.randn(784, 128))\n",
    "# b = Tensor(np.random.randn(1, 128))\n",
    "\n",
    "# x1 = Tensor(np.array(3), requires_grad=True)\n",
    "# x2 = Tensor(np.array(7), requires_grad=False)\n",
    "\n",
    "# interim = ((x1 + x1) + x2)\n",
    "# f = interim * interim\n",
    "\n",
    "# $(2x_1 + x_2)^2 = 4x_1^2 + 4x_1x_2 +x_2^2$ \n",
    "\n",
    "# $\\frac{\\partial{df}}{\\partial{x_1}} =8x_1 + 4x_2 = 24 + 28 = 52$\n",
    "\n",
    "# $\\frac{\\partial{df}}{\\partial{x_2}} =4x_1 + 2x_2 = 12 + 14 = 26$\n",
    "\n",
    "# f.backward()\n",
    "\n",
    "# x1.grad\n",
    "\n",
    "# model = NeuralNet()\n",
    "# model.add(nn.Layer(n_inputs=784, n_outputs=128))\n",
    "# model.add(nn.Layer(n_inputs=128, n_outputs=10))\n",
    "# y_pred = model.forward(mnist.X_train)\n",
    "\n",
    "# l = \n",
    "# for layer in model.layers[::-1]:\n",
    "#     print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNet()\n",
    "# print(model.fc1.shape)\n",
    "# print(model.fc2.shape)\n",
    "# epochs = 400\n",
    "# learning_rate = 0.99\n",
    "# m = mnist.X_train.shape[0]\n",
    "# for epoch in np.arange(epochs):\n",
    "    \n",
    "#     # Feedforward\n",
    "#     y_pred, z2, a1, z1 = model.forward(mnist.X_train)\n",
    "    \n",
    "#     # Calculate cost\n",
    "#     loss = cross_entropy_loss(y_pred, mnist.y_train)\n",
    "    \n",
    "#     # Backprop\n",
    "#     dw2, db2, dw1, db1 = model.backward(mnist.X_train, mnist.y_train, [y_pred, z2, a1, z1])\n",
    "    \n",
    "#     # Update weights\n",
    "#     model.fc2.W = model.fc2.W - learning_rate * 1 / m * dw2\n",
    "#     model.fc1.W = model.fc1.W - learning_rate * 1 / m * dw1\n",
    "#     model.fc2.b = model.fc2.b - learning_rate * 1 / m * db2\n",
    "#     model.fc1.b = model.fc1.b - learning_rate * 1 / m * db1\n",
    "    \n",
    "#     acc = accuracy_score(mnist.y_train, y_pred)\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'epoch = {epoch}, loss = {loss:.3f}, TRAIN ACC = {acc:.3f}')\n",
    "#     epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = Graph()\n",
    "\n",
    "# g.set_as_default()\n",
    "\n",
    "# X = Tensor(mnist.X_train)\n",
    "\n",
    "# X + W\n",
    "\n",
    "# W = Variable(np.random.rand(784, 128))\n",
    "\n",
    "# b = Variable(np.zeros(shape=(1, 128)))\n",
    "\n",
    "# z = add(matmul(X, W), b)\n",
    "\n",
    "# z.input_nodes[0].input_nodes[0]\n",
    "\n",
    "# z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fault_tolerant_ml.ml.ops import tensor as ft\n",
    "\n",
    "# g = ft.Graph()\n",
    "# g.set_as_default()\n",
    "# X = ft.Tensor(mnist.X_train)\n",
    "# y = ft.Tensor(mnist.y_train)\n",
    "\n",
    "# W = ft.Tensor(np.random.randn(784, 128))\n",
    "\n",
    "# b = ft.Tensor(np.zeros((1, 784)))\n",
    "\n",
    "# a = ft.matmul(X, W)\n",
    "# # z = ft.add(W, b)\n",
    "\n",
    "# g.operations\n",
    "\n",
    "# def evalulate(f):\n",
    "#     val = []\n",
    "#     for i, op in enumerate(f.operations):\n",
    "#         print(*op.input_nodes)\n",
    "#         val.append(op.compute(*op.input_nodes))\n",
    "\n",
    "# def traverse(f):\n",
    "    \n",
    "#     operations = []\n",
    "#     def recurse(node):\n",
    "#         if isinstance(node, ft.Operation):\n",
    "#             for input_node in node.input_nodes:\n",
    "#                 recurse(input_node)\n",
    "#         operations.append(node)\n",
    "            \n",
    "#     recurse(f)\n",
    "#     return operations\n",
    "\n",
    "# g = ft.Graph()\n",
    "# g.set_as_default()\n",
    "# x1 = ft.Tensor(np.array(3))\n",
    "# x2 = ft.Tensor(np.array(7))\n",
    "\n",
    "# f = ft.square(ft.add(ft.add(x1, x1), x2))\n",
    "\n",
    "# traverse(f)\n",
    "\n",
    "# g.operations[0].input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalulate(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [ (\"z1\", \"add\", (\"x1\",\"x1\")),\n",
    "# (\"z2\", \"add\", (\"z1\",\"x2\")),\n",
    "# (\"f\", \"square\", (\"z2\",)) ]\n",
    "\n",
    "# G = { \"add\" : lambda a,b: a+b,\n",
    "# \"square\": lambda a:a*a }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = { \"x1\" : 3, \"x2\" : 7 }\n",
    "\n",
    "# for step in l:\n",
    "#     print(val)\n",
    "#     var, op_name, func = step\n",
    "#     lookup = list(map(val.get, func))\n",
    "#     val[var] = G[op_name](*lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DG = { \"add\" : [ (lambda a,b: 1), (lambda a,b: 1) ],\n",
    "# \"square\": [ lambda a:2*a ] }\n",
    "\n",
    "# delta={}\n",
    "# delta[\"f\"] = 1\n",
    "# for step in l[::-1]:\n",
    "#     var, op_name, func = step\n",
    "#     for op in DG[op_name]:\n",
    "#         if var not in delta:\n",
    "#             delta[var] = 0\n",
    "#         lookup = list(map(val.get, func))\n",
    "#         print(lookup)\n",
    "#         delta[var] = delta[var] + DG[op_name](*lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tensor(object):\n",
    "    \n",
    "#     def __init__(self, data: np.ndarray, depends_on=None):\n",
    "        \n",
    "#         self.depends_on = depends_on or []\n",
    "#         self.data = data\n",
    "            \n",
    "#     def __add__(self, other):\n",
    "#         return Tensor(self.data + other.data, depends_on=[self, other])\n",
    "    \n",
    "#     def __pow__(self, p):\n",
    "#         data = self.data ** p \n",
    "#         return Tensor(data, depends_on=[self])\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f\"Tensor({self.data}, dtype={self.data.dtype})\"\n",
    "\n",
    "\n",
    "# y1 = Tensor(np.array(3))\n",
    "# y2 = Tensor(np.array(7))\n",
    "# z1 = y1 + y1\n",
    "# z2 = z1 ** 2\n",
    "\n",
    "# z2.depends_on\n",
    "\n",
    "# layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyn_fed.activations.activation import Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyn_fed.losses.loss_fns import CrossEntropyLoss, MSELoss\n",
    "from dyn_fed.models import Model\n",
    "from dyn_fed.layers import Layer\n",
    "from dyn_fed.optimizers import SGD\n",
    "from dyn_fed.metrics import accuracy_scorev2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=100):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    for epoch in np.arange(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for start in range(0, X_train.shape[0], batch_size):\n",
    "            end = start + batch_size\n",
    "\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Feedforward\n",
    "            y_pred = model.forward(X_batch)\n",
    "            \n",
    "            # print(y_pred[1])\n",
    "\n",
    "            # Calculate loss\n",
    "            batch_loss = loss.loss(y_batch, y_pred, reduce=True).data\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.compute_gradients(model, y_batch, y_pred)\n",
    "\n",
    "            # Update gradients\n",
    "            optimizer.apply_gradients(model)\n",
    "\n",
    "            epoch_loss = epoch_loss + batch_loss\n",
    "            n_batches += 1\n",
    "\n",
    "        epoch_loss = epoch_loss / n_batches\n",
    "\n",
    "        # Calculate accuracy\n",
    "        y_pred_train = model.forward(X_train)\n",
    "        train_acc = accuracy_scorev2(y_train.data, y_pred_train.data)\n",
    "        train_accs.append(train_acc)\n",
    "        # Test accuracy\n",
    "        y_pred_test = model.forward(X_test)\n",
    "        test_acc = accuracy_scorev2(y_test.data, y_pred_test.data)\n",
    "        test_loss = loss.loss(y_test.data, y_pred_test.data, reduce=True)\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Iteration {epoch}: Loss={epoch_loss:.4f}, train accuracy={train_acc:.4f}, test accuracy={test_acc:.4f}\")\n",
    "        epoch += 1\n",
    "        \n",
    "    return epoch_loss, train_losses, test_losses, train_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "loss = CrossEntropyLoss()\n",
    "# loss = MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = SGD(loss, learning_rate=0.1)\n",
    "\n",
    "# Define model\n",
    "model = Model(optimizer=optimizer)\n",
    "l1 = Layer(784, 10, activation=\"sigmoid\")\n",
    "# l2 = Lay(128, 128)\n",
    "# l3 = Layer(128, 10)\n",
    "# Add layers\n",
    "model.add([l1])\n",
    "\n",
    "# Tensorize numpy arrays\n",
    "X_train = mnist.X_train\n",
    "y_train = mnist.y_train\n",
    "X_test = mnist.X_test\n",
    "y_test = mnist.y_test\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.8 ms, sys: 975 µs, total: 74.8 ms\n",
      "Wall time: 12.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time y_pred = model.forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss=0.2017, train accuracy=0.8612, test accuracy=0.8709\n",
      "Iteration 1: Loss=0.1542, train accuracy=0.8778, test accuracy=0.8881\n",
      "Iteration 2: Loss=0.1458, train accuracy=0.8852, test accuracy=0.8949\n",
      "Iteration 3: Loss=0.1421, train accuracy=0.8898, test accuracy=0.8982\n",
      "Iteration 4: Loss=0.1401, train accuracy=0.8928, test accuracy=0.9002\n",
      "Iteration 5: Loss=0.1388, train accuracy=0.8953, test accuracy=0.9031\n",
      "Iteration 6: Loss=0.1380, train accuracy=0.8971, test accuracy=0.9052\n",
      "Iteration 7: Loss=0.1375, train accuracy=0.8987, test accuracy=0.9064\n",
      "Iteration 8: Loss=0.1372, train accuracy=0.9001, test accuracy=0.9078\n",
      "Iteration 9: Loss=0.1370, train accuracy=0.9013, test accuracy=0.9088\n",
      "Iteration 10: Loss=0.1368, train accuracy=0.9025, test accuracy=0.9097\n",
      "Iteration 11: Loss=0.1368, train accuracy=0.9034, test accuracy=0.9106\n",
      "Iteration 12: Loss=0.1368, train accuracy=0.9042, test accuracy=0.9119\n",
      "Iteration 13: Loss=0.1368, train accuracy=0.9050, test accuracy=0.9122\n",
      "Iteration 14: Loss=0.1368, train accuracy=0.9056, test accuracy=0.9126\n",
      "Iteration 15: Loss=0.1369, train accuracy=0.9063, test accuracy=0.9126\n",
      "Iteration 16: Loss=0.1370, train accuracy=0.9068, test accuracy=0.9132\n",
      "Iteration 17: Loss=0.1371, train accuracy=0.9073, test accuracy=0.9135\n",
      "Iteration 18: Loss=0.1373, train accuracy=0.9076, test accuracy=0.9137\n",
      "Iteration 19: Loss=0.1374, train accuracy=0.9081, test accuracy=0.9140\n",
      "Iteration 20: Loss=0.1375, train accuracy=0.9087, test accuracy=0.9143\n",
      "Iteration 21: Loss=0.1377, train accuracy=0.9091, test accuracy=0.9148\n",
      "Iteration 22: Loss=0.1378, train accuracy=0.9096, test accuracy=0.9150\n",
      "Iteration 23: Loss=0.1380, train accuracy=0.9101, test accuracy=0.9155\n",
      "Iteration 24: Loss=0.1381, train accuracy=0.9105, test accuracy=0.9159\n",
      "Iteration 25: Loss=0.1383, train accuracy=0.9108, test accuracy=0.9163\n",
      "Iteration 26: Loss=0.1385, train accuracy=0.9111, test accuracy=0.9165\n",
      "Iteration 27: Loss=0.1386, train accuracy=0.9114, test accuracy=0.9164\n",
      "Iteration 28: Loss=0.1388, train accuracy=0.9118, test accuracy=0.9166\n",
      "Iteration 29: Loss=0.1389, train accuracy=0.9119, test accuracy=0.9170\n",
      "Iteration 30: Loss=0.1391, train accuracy=0.9120, test accuracy=0.9171\n",
      "Iteration 31: Loss=0.1393, train accuracy=0.9123, test accuracy=0.9171\n",
      "Iteration 32: Loss=0.1394, train accuracy=0.9125, test accuracy=0.9172\n",
      "Iteration 33: Loss=0.1396, train accuracy=0.9127, test accuracy=0.9173\n",
      "Iteration 34: Loss=0.1398, train accuracy=0.9129, test accuracy=0.9177\n",
      "Iteration 35: Loss=0.1399, train accuracy=0.9131, test accuracy=0.9174\n",
      "Iteration 36: Loss=0.1401, train accuracy=0.9134, test accuracy=0.9175\n",
      "Iteration 37: Loss=0.1402, train accuracy=0.9135, test accuracy=0.9172\n",
      "Iteration 38: Loss=0.1404, train accuracy=0.9139, test accuracy=0.9174\n",
      "Iteration 39: Loss=0.1405, train accuracy=0.9141, test accuracy=0.9175\n",
      "Iteration 40: Loss=0.1407, train accuracy=0.9144, test accuracy=0.9177\n",
      "Iteration 41: Loss=0.1409, train accuracy=0.9147, test accuracy=0.9180\n",
      "Iteration 42: Loss=0.1410, train accuracy=0.9149, test accuracy=0.9181\n",
      "Iteration 43: Loss=0.1412, train accuracy=0.9152, test accuracy=0.9180\n",
      "Iteration 44: Loss=0.1413, train accuracy=0.9154, test accuracy=0.9180\n",
      "Iteration 45: Loss=0.1415, train accuracy=0.9156, test accuracy=0.9180\n",
      "Iteration 46: Loss=0.1416, train accuracy=0.9157, test accuracy=0.9179\n",
      "Iteration 47: Loss=0.1418, train accuracy=0.9159, test accuracy=0.9181\n",
      "Iteration 48: Loss=0.1419, train accuracy=0.9160, test accuracy=0.9181\n",
      "Iteration 49: Loss=0.1421, train accuracy=0.9160, test accuracy=0.9182\n",
      "Iteration 50: Loss=0.1422, train accuracy=0.9161, test accuracy=0.9185\n",
      "Iteration 51: Loss=0.1424, train accuracy=0.9162, test accuracy=0.9186\n",
      "Iteration 52: Loss=0.1425, train accuracy=0.9163, test accuracy=0.9186\n",
      "Iteration 53: Loss=0.1427, train accuracy=0.9165, test accuracy=0.9189\n",
      "Iteration 54: Loss=0.1428, train accuracy=0.9167, test accuracy=0.9188\n",
      "Iteration 55: Loss=0.1429, train accuracy=0.9169, test accuracy=0.9189\n",
      "Iteration 56: Loss=0.1431, train accuracy=0.9170, test accuracy=0.9190\n",
      "Iteration 57: Loss=0.1432, train accuracy=0.9171, test accuracy=0.9191\n",
      "Iteration 58: Loss=0.1434, train accuracy=0.9171, test accuracy=0.9190\n",
      "Iteration 59: Loss=0.1435, train accuracy=0.9172, test accuracy=0.9190\n",
      "Iteration 60: Loss=0.1436, train accuracy=0.9174, test accuracy=0.9188\n",
      "Iteration 61: Loss=0.1438, train accuracy=0.9175, test accuracy=0.9189\n",
      "Iteration 62: Loss=0.1439, train accuracy=0.9176, test accuracy=0.9186\n",
      "Iteration 63: Loss=0.1440, train accuracy=0.9177, test accuracy=0.9186\n",
      "Iteration 64: Loss=0.1442, train accuracy=0.9179, test accuracy=0.9186\n",
      "Iteration 65: Loss=0.1443, train accuracy=0.9180, test accuracy=0.9188\n",
      "Iteration 66: Loss=0.1444, train accuracy=0.9180, test accuracy=0.9187\n",
      "Iteration 67: Loss=0.1446, train accuracy=0.9180, test accuracy=0.9186\n",
      "Iteration 68: Loss=0.1447, train accuracy=0.9181, test accuracy=0.9189\n",
      "Iteration 69: Loss=0.1448, train accuracy=0.9182, test accuracy=0.9187\n",
      "Iteration 70: Loss=0.1449, train accuracy=0.9183, test accuracy=0.9186\n",
      "Iteration 71: Loss=0.1451, train accuracy=0.9183, test accuracy=0.9185\n",
      "Iteration 72: Loss=0.1452, train accuracy=0.9184, test accuracy=0.9186\n",
      "Iteration 73: Loss=0.1453, train accuracy=0.9185, test accuracy=0.9187\n",
      "Iteration 74: Loss=0.1455, train accuracy=0.9187, test accuracy=0.9189\n",
      "Iteration 75: Loss=0.1456, train accuracy=0.9188, test accuracy=0.9192\n",
      "Iteration 76: Loss=0.1457, train accuracy=0.9189, test accuracy=0.9194\n",
      "Iteration 77: Loss=0.1458, train accuracy=0.9191, test accuracy=0.9194\n",
      "Iteration 78: Loss=0.1459, train accuracy=0.9192, test accuracy=0.9196\n",
      "Iteration 79: Loss=0.1461, train accuracy=0.9192, test accuracy=0.9197\n",
      "Iteration 80: Loss=0.1462, train accuracy=0.9193, test accuracy=0.9199\n",
      "Iteration 81: Loss=0.1463, train accuracy=0.9194, test accuracy=0.9199\n",
      "Iteration 82: Loss=0.1464, train accuracy=0.9195, test accuracy=0.9199\n",
      "Iteration 83: Loss=0.1465, train accuracy=0.9196, test accuracy=0.9198\n",
      "Iteration 84: Loss=0.1467, train accuracy=0.9196, test accuracy=0.9199\n",
      "Iteration 85: Loss=0.1468, train accuracy=0.9197, test accuracy=0.9199\n",
      "Iteration 86: Loss=0.1469, train accuracy=0.9198, test accuracy=0.9200\n",
      "Iteration 87: Loss=0.1470, train accuracy=0.9198, test accuracy=0.9200\n",
      "Iteration 88: Loss=0.1471, train accuracy=0.9198, test accuracy=0.9199\n",
      "Iteration 89: Loss=0.1472, train accuracy=0.9199, test accuracy=0.9200\n",
      "Iteration 90: Loss=0.1473, train accuracy=0.9200, test accuracy=0.9202\n",
      "Iteration 91: Loss=0.1475, train accuracy=0.9201, test accuracy=0.9203\n",
      "Iteration 92: Loss=0.1476, train accuracy=0.9201, test accuracy=0.9203\n",
      "Iteration 93: Loss=0.1477, train accuracy=0.9202, test accuracy=0.9204\n",
      "Iteration 94: Loss=0.1478, train accuracy=0.9203, test accuracy=0.9205\n",
      "Iteration 95: Loss=0.1479, train accuracy=0.9203, test accuracy=0.9206\n",
      "Iteration 96: Loss=0.1480, train accuracy=0.9204, test accuracy=0.9206\n",
      "Iteration 97: Loss=0.1481, train accuracy=0.9205, test accuracy=0.9206\n",
      "Iteration 98: Loss=0.1482, train accuracy=0.9206, test accuracy=0.9207\n",
      "Iteration 99: Loss=0.1483, train accuracy=0.9206, test accuracy=0.9206\n",
      "CPU times: user 3min 42s, sys: 986 ms, total: 3min 43s\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, train_losses, test_losses, train_accs, test_accs = (\n",
    "    train(model, X_train, y_train, X_test, y_test, batch_size=batch_size, epochs=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14c79fef0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(test_losses, label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loss = MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "f_optimizer = SGD(f_loss, learning_rate=0.001)\n",
    "\n",
    "# Define model\n",
    "f_model = Model(optimizer=optimizer)\n",
    "f_l1 = Layer(784, 10, activation=\"linear\")\n",
    "# l2 = Lay(128, 128)\n",
    "# l3 = Layer(128, 10)\n",
    "# Add layers\n",
    "f_model.add([f_l1])\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss=0.0258, train accuracy=0.7375, test accuracy=0.7279\n",
      "Iteration 1: Loss=0.0210, train accuracy=0.7761, test accuracy=0.7645\n",
      "Iteration 2: Loss=0.0198, train accuracy=0.7904, test accuracy=0.7785\n",
      "Iteration 3: Loss=0.0192, train accuracy=0.7976, test accuracy=0.7860\n",
      "Iteration 4: Loss=0.0188, train accuracy=0.8028, test accuracy=0.7913\n",
      "Iteration 5: Loss=0.0185, train accuracy=0.8060, test accuracy=0.7947\n",
      "Iteration 6: Loss=0.0183, train accuracy=0.8088, test accuracy=0.7982\n",
      "Iteration 7: Loss=0.0181, train accuracy=0.8111, test accuracy=0.8006\n",
      "Iteration 8: Loss=0.0180, train accuracy=0.8127, test accuracy=0.8016\n",
      "Iteration 9: Loss=0.0179, train accuracy=0.8135, test accuracy=0.8035\n",
      "Iteration 10: Loss=0.0178, train accuracy=0.8146, test accuracy=0.8047\n",
      "Iteration 11: Loss=0.0177, train accuracy=0.8155, test accuracy=0.8048\n",
      "Iteration 12: Loss=0.0176, train accuracy=0.8161, test accuracy=0.8055\n",
      "Iteration 13: Loss=0.0176, train accuracy=0.8169, test accuracy=0.8060\n",
      "Iteration 14: Loss=0.0175, train accuracy=0.8174, test accuracy=0.8068\n",
      "Iteration 15: Loss=0.0175, train accuracy=0.8182, test accuracy=0.8069\n",
      "Iteration 16: Loss=0.0175, train accuracy=0.8184, test accuracy=0.8069\n",
      "Iteration 17: Loss=0.0174, train accuracy=0.8189, test accuracy=0.8070\n",
      "Iteration 18: Loss=0.0174, train accuracy=0.8192, test accuracy=0.8074\n",
      "Iteration 19: Loss=0.0174, train accuracy=0.8194, test accuracy=0.8078\n",
      "Iteration 20: Loss=0.0173, train accuracy=0.8198, test accuracy=0.8083\n",
      "Iteration 21: Loss=0.0173, train accuracy=0.8201, test accuracy=0.8087\n",
      "Iteration 22: Loss=0.0173, train accuracy=0.8203, test accuracy=0.8089\n",
      "Iteration 23: Loss=0.0173, train accuracy=0.8206, test accuracy=0.8092\n",
      "Iteration 24: Loss=0.0173, train accuracy=0.8209, test accuracy=0.8095\n",
      "Iteration 25: Loss=0.0172, train accuracy=0.8210, test accuracy=0.8098\n",
      "Iteration 26: Loss=0.0172, train accuracy=0.8212, test accuracy=0.8101\n",
      "Iteration 27: Loss=0.0172, train accuracy=0.8215, test accuracy=0.8108\n",
      "Iteration 28: Loss=0.0172, train accuracy=0.8214, test accuracy=0.8112\n",
      "Iteration 29: Loss=0.0172, train accuracy=0.8217, test accuracy=0.8113\n",
      "Iteration 30: Loss=0.0172, train accuracy=0.8218, test accuracy=0.8113\n",
      "Iteration 31: Loss=0.0172, train accuracy=0.8220, test accuracy=0.8110\n",
      "Iteration 32: Loss=0.0172, train accuracy=0.8222, test accuracy=0.8112\n",
      "Iteration 33: Loss=0.0171, train accuracy=0.8225, test accuracy=0.8111\n",
      "Iteration 34: Loss=0.0171, train accuracy=0.8226, test accuracy=0.8115\n",
      "Iteration 35: Loss=0.0171, train accuracy=0.8227, test accuracy=0.8116\n",
      "Iteration 36: Loss=0.0171, train accuracy=0.8227, test accuracy=0.8113\n",
      "Iteration 37: Loss=0.0171, train accuracy=0.8229, test accuracy=0.8113\n",
      "Iteration 38: Loss=0.0171, train accuracy=0.8230, test accuracy=0.8114\n",
      "Iteration 39: Loss=0.0171, train accuracy=0.8231, test accuracy=0.8116\n",
      "Iteration 40: Loss=0.0171, train accuracy=0.8233, test accuracy=0.8117\n",
      "Iteration 41: Loss=0.0171, train accuracy=0.8235, test accuracy=0.8120\n",
      "Iteration 42: Loss=0.0171, train accuracy=0.8238, test accuracy=0.8121\n",
      "Iteration 43: Loss=0.0171, train accuracy=0.8238, test accuracy=0.8125\n",
      "Iteration 44: Loss=0.0171, train accuracy=0.8240, test accuracy=0.8126\n",
      "Iteration 45: Loss=0.0171, train accuracy=0.8241, test accuracy=0.8126\n",
      "Iteration 46: Loss=0.0170, train accuracy=0.8241, test accuracy=0.8125\n",
      "Iteration 47: Loss=0.0170, train accuracy=0.8241, test accuracy=0.8126\n",
      "Iteration 48: Loss=0.0170, train accuracy=0.8242, test accuracy=0.8127\n",
      "Iteration 49: Loss=0.0170, train accuracy=0.8243, test accuracy=0.8129\n",
      "Iteration 50: Loss=0.0170, train accuracy=0.8244, test accuracy=0.8129\n",
      "Iteration 51: Loss=0.0170, train accuracy=0.8244, test accuracy=0.8129\n",
      "Iteration 52: Loss=0.0170, train accuracy=0.8245, test accuracy=0.8130\n",
      "Iteration 53: Loss=0.0170, train accuracy=0.8246, test accuracy=0.8129\n",
      "Iteration 54: Loss=0.0170, train accuracy=0.8247, test accuracy=0.8131\n",
      "Iteration 55: Loss=0.0170, train accuracy=0.8247, test accuracy=0.8131\n",
      "Iteration 56: Loss=0.0170, train accuracy=0.8248, test accuracy=0.8131\n",
      "Iteration 57: Loss=0.0170, train accuracy=0.8248, test accuracy=0.8131\n",
      "Iteration 58: Loss=0.0170, train accuracy=0.8249, test accuracy=0.8132\n",
      "Iteration 59: Loss=0.0170, train accuracy=0.8249, test accuracy=0.8132\n",
      "Iteration 60: Loss=0.0170, train accuracy=0.8250, test accuracy=0.8131\n",
      "Iteration 61: Loss=0.0170, train accuracy=0.8250, test accuracy=0.8131\n",
      "Iteration 62: Loss=0.0170, train accuracy=0.8252, test accuracy=0.8131\n",
      "Iteration 63: Loss=0.0170, train accuracy=0.8253, test accuracy=0.8132\n",
      "Iteration 64: Loss=0.0170, train accuracy=0.8254, test accuracy=0.8130\n",
      "Iteration 65: Loss=0.0170, train accuracy=0.8255, test accuracy=0.8128\n",
      "Iteration 66: Loss=0.0170, train accuracy=0.8256, test accuracy=0.8126\n",
      "Iteration 67: Loss=0.0170, train accuracy=0.8256, test accuracy=0.8125\n",
      "Iteration 68: Loss=0.0170, train accuracy=0.8257, test accuracy=0.8126\n",
      "Iteration 69: Loss=0.0170, train accuracy=0.8258, test accuracy=0.8126\n",
      "Iteration 70: Loss=0.0170, train accuracy=0.8259, test accuracy=0.8127\n",
      "Iteration 71: Loss=0.0170, train accuracy=0.8259, test accuracy=0.8125\n",
      "Iteration 72: Loss=0.0169, train accuracy=0.8259, test accuracy=0.8124\n",
      "Iteration 73: Loss=0.0169, train accuracy=0.8259, test accuracy=0.8124\n",
      "Iteration 74: Loss=0.0169, train accuracy=0.8259, test accuracy=0.8124\n",
      "Iteration 75: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 76: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 77: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 78: Loss=0.0169, train accuracy=0.8260, test accuracy=0.8124\n",
      "Iteration 79: Loss=0.0169, train accuracy=0.8261, test accuracy=0.8127\n",
      "Iteration 80: Loss=0.0169, train accuracy=0.8261, test accuracy=0.8126\n",
      "Iteration 81: Loss=0.0169, train accuracy=0.8262, test accuracy=0.8127\n",
      "Iteration 82: Loss=0.0169, train accuracy=0.8263, test accuracy=0.8130\n",
      "Iteration 83: Loss=0.0169, train accuracy=0.8262, test accuracy=0.8129\n",
      "Iteration 84: Loss=0.0169, train accuracy=0.8263, test accuracy=0.8129\n",
      "Iteration 85: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8128\n",
      "Iteration 86: Loss=0.0169, train accuracy=0.8263, test accuracy=0.8127\n",
      "Iteration 87: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8127\n",
      "Iteration 88: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 89: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8125\n",
      "Iteration 90: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 91: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 92: Loss=0.0169, train accuracy=0.8265, test accuracy=0.8126\n",
      "Iteration 93: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8124\n",
      "Iteration 94: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8126\n",
      "Iteration 95: Loss=0.0169, train accuracy=0.8264, test accuracy=0.8125\n",
      "Iteration 96: Loss=0.0169, train accuracy=0.8265, test accuracy=0.8126\n",
      "Iteration 97: Loss=0.0169, train accuracy=0.8266, test accuracy=0.8127\n",
      "Iteration 98: Loss=0.0169, train accuracy=0.8266, test accuracy=0.8126\n",
      "Iteration 99: Loss=0.0169, train accuracy=0.8267, test accuracy=0.8127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01689868272691965"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(\n",
    "    f_model,\n",
    "    fashion_mnist.X_train,\n",
    "    fashion_mnist.y_train,\n",
    "    fashion_mnist.X_test,\n",
    "    fashion_mnist.y_test,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ftml]",
   "language": "python",
   "name": "conda-env-ftml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
